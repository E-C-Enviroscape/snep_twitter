---
title: "What ecosystem services do people in the SNEP region care about?"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## this chunk loads all of the packages we need

## for twitter word search
library(rtweet) # install.packages("rtweet")

## load text mining library
library(tidytext) # install.packages("tidytext")

## plotting packages
library(plyr) # install.packages("plyr")
library(readr) # install.packages("readr")
library(tidyverse, igraph) # install.packages("tidyverse", "igraph")

# install.packages('devtools')
devtools::install_github('bbc/bbplot')
library(bbplot)

```

We picked words that might indicate general interest, concern, or focus of 
Twitter users in the SNEP region (Westerly, RI to Cape Cod, MA).

We are examining Twitter data one day every week from March to August 2020 to 
determine which words (representing ecosystem services) are the most popular. We will be using the 
[rtweet package](https://rtweet.info/) in R to do this.

|Twitter search words|   	      |   	   |     	 |
|--------------------|----------|--------|-------|
|swimming	           |hiking	  |fishing |ferry  |
|boating	           |kayaking  |ocean	 |beach  |
|wave	               |wetland	  |seaweed |river	 |
|bay	               |forest	  |seafood |tide   |
|red tide	           |nor'easter|flooding|drought|
|beach closure       |seal	    |   	   |   	   |

***

```{r stopwords, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

## running list of words that contain snep words but aren't of interest

mystopwords <- tibble(word = c("driver", "drivers", "hitchhiking", "sealed", 
                               "unwavering", "microwave", "waverly", "waved", 
                               "#ebay", "#bluewave2020", "bayek", "#busdriver", 
                               "bayer", "unsealed", "smithrivera", "seales", 
                               "rainforests", "rainforest", "rivera", "riverside", 
                               "#bluewave", "bayada", "#deliverydriver", "kumbaya", 
                               "antidepressants", "beaches", "seals"))

```


```{r snep_twitter_today, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

## need google maps API to get georeferenced tweets

## search for 18,000 tweets in RI and MA; note: "dt" stands for "daily tweets"
#dt <- search_tweets("lang:en", geocode = lookup_coords("rhode island", "massachusetts", apikey = "AIzaSyDekm4ATG_9Ir0N1VGuNo8accyKrnttbDU" ), n = 18000, include_rts = FALSE)

## remove http elements manually
dt$text <- gsub("http.*","",  dt$text)
dt$text <- gsub("https.*","", dt$text)

## create list of words and make tidy
dt_words <- dt %>%
  dplyr::select(text) %>%
  unnest_tokens(word, text, token = "tweets") %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word , "[a-z]"))

## filter out stopwords and emojis
dt_words$word <- sapply(dt_words$word,function(row) iconv(row, "latin1", "ASCII", sub=""))

dt_words <- anti_join(dt_words, mystopwords, 
                           by = "word")

## filter out all other words besides snep_words
dt_snep <- dt_words %>% 
  filter(str_detect(word, "swimming|hiking|boating|kayaking|fishing|ferry|ocean|beach|wave|wetland|seaweed|river|bay|forest|seafood|beach closure|red tide|noreaster|flooding|drought|tide|seal"))

## filter out usernames
dt_snep <- str_remove(dt_snep$word, "@.*") %>%
  data.frame(lapply(dt_snep, as.character), stringsAsFactors=FALSE)

## remove blanks
dt_snep <- dt_snep[!(is.na(dt_snep$.) | dt_snep$.==""), ]

## select single column and rename
dt_snep <- select(dt_snep, .) %>%
  rename(word = 1)

## add a date column and save for adding to future months!
current_date <- Sys.Date() ## calling the current date

dt_snep$date <- current_date ## putting the current date in a column named "date"

word_search_FileName <- paste("data/daily_data/words/snep_twitter_words_", current_date, 
                              ".csv", sep="") ## creating an object for the filename with current date

write.csv(dt_snep, word_search_FileName, row.names = FALSE) ## writing the data file


## summarize by word frequency
dt_freq <- dt_snep %>%
  group_by(word) %>%
  count(word, sort = TRUE) %>%
  left_join(dt_snep %>%
              group_by(word) %>%
              summarise())

word_freq_FileName <- paste("data/daily_data/freq/snep_twitter_freq_", current_date, 
                            ".csv", sep="") ## creating an object for the filename with current date

write.csv(dt_freq, word_freq_FileName, row.names = FALSE) ## writing the data file

## plot top words
dt_plot <- dt_snep %>%
  count(word, sort = TRUE) %>%
  top_n(14) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(y = "Count",
      x = "Top SNEP Key Words",
      title = paste(current_date, ": Count of SNEP key words found in tweets"))

dt_plot

word_freq_plot_FileName <- paste("figures/daily_figures/snep_twitter_word_freq_plot_", 
                                 current_date, ".png", sep="") ## creating an object for the filename with current date

ggsave(word_freq_plot_FileName, dpi = 200)

```


```{r snep_twitter_all, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)

## merge all of the weekly data into a single data frame
words_list <- list.files(path = "data/daily_data/words",  pattern = "*.csv", 
                         full.names = TRUE)

all_snep <- ldply(words_list, read_csv)

## summarize by word frequency
allsnep_freq <- all_snep %>%
  group_by(word) %>%
  count(word, sort = TRUE) %>%
  left_join(all_snep %>%
             group_by(word) %>%
              summarise())

write.csv(allsnep_freq,"data/allsnep_freq.csv", row.names = FALSE)

## plot top words
allsnep_plot <- all_snep %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(fill="#1380A1") +
  geom_hline(yintercept = 0, size = 1, colour="#333333") +
  bbc_style() +
  theme(plot.title = ggplot2::element_text(size = 20), plot.subtitle = 
          ggplot2::element_text(size = 16), axis.text = ggplot2::element_text(size = 14)) +
  geom_label(aes(x = word, y = n, label = round(n, 0)),
             hjust = 1, 
             vjust = 0.5, 
             colour = "white", 
             fill = NA, 
             label.size = NA, 
             family="Helvetica", 
             size = 4) +
  geom_label(aes(x = 2, y = 30, label = "The data presented here is derived from publicly available Twitter data obtained 
via the rtweet package in R and used in accordance with the Twitter Privacy Policy."),
                           hjust = 0, 
                           vjust = 0.5, 
                           colour = "#555555", 
                           fill = "white", 
                           label.size = NA, 
                           family="Helvetica", 
                           size = 2) +
  xlab(NULL) +
  coord_flip() +
      labs(y = "Count",
      x = "Top SNEP Key Words",
      title = "Count of SNEP key words found in tweets",
       subtitle = "March 6 - 20, 2020")

allsnep_plot
ggsave("figures/allsnep_plot.png", dpi = 200)


## combine daily plots
allsnep_daily_plot <- all_snep %>%
  group_by(word, date) %>%
  count(word, sort = FALSE) %>%
  arrange(date, -n, word) %>%
  filter(n != "1") %>%
  top_n(15) %>%
  #mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  facet_wrap(~date) +
  xlab(NULL) +
  coord_flip() +
      labs(y = "Count",
      x = "Top SNEP Key Words",
      title = "March 6 - 20, 2020: Count of SNEP key words found in tweets")

allsnep_daily_plot
ggsave("figures/allsnep_daily_plot.png", dpi = 200)

```

## So, what do people in the SNEP region care about?

![](figures/allsnep_plot.png)
***

## Daily plots
![](figures/allsnep_daily_plot.png)